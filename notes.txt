Snowflake interface (snowsight):
--------------------------------
> Worksheets: Write/run queries and code

Architecture:
-------------
> Data is compressed and stored in blobs (AWS S3, Azure Blob Storage, GCP Buckets)
> Virtual warehouses: A set of virtual compute engines query the data stored in blobs (also perform MPP wherein multiple servers come together to process big data)
> Cloud providers help to manage all of this: Managing infrastructure, access control, security, optimizer and metadata
> Virtual warehouse sizes (no. of servers): 
    > XS - 1, S - 2, M - 4, L - 8, XL - 16, 4XL - 128
    > Billed by minute (min. 1 minute)
    > Multi-clustering: When you a multiple queries to execute but 1 virtual warehouse is not sufficient, then you can use multi clustering which brings in multiple virtual warehouses together (like a cluster) so that queries dont get queued up

Scaling policy
--------------
> Scaling is required when the workload is high (more queries to be processed) 
> Policies
    > Standard:
        > This is the default scaling policy
        > Prevents/minimizes queuing by favoring starting additional clusters over conserving credits
        > Cluster starts immeditely when either a query is queued or the system detects that there are more queries than can be executed by the currently available clusters
        > Cluster shuts down after 2 to 3 consecutive successful checks (performed at 1 minute intervals) which determine whether the load on the least-loaded cluster could be redistributed to the other clusters 
    > Economy
        > Conserves credits by favoring keeping running clusters fully loaded rather than starting additional clusters
        > May result in queries being queued and taking longer to complete
        > Only if the system estimates there's enough query load to keep the cluster busy for at least 6 minutes
        > After 5 to 6 consecutive successful checks

Snowflake editions
------------------
> Standard: Introductory level
> Enterprise: Additional features for the needs of largescale enterprises
> Business Critical: Even higher levels of data protection for organisations with extremely sensitive data
> Virtual Private: Highest level of security

Snowflake pricing
-----------------
> Pay only what you need
> Scalable amount of storage at affordable cloud price
> Pricing depending on the region and cloud platform
> Compute and storage costs decoupled
> Storage
    > Monthly storage fees
    > Based on average storage used per month
    > Cost calculated after compression
    > If we pay upfront for pre-defined capacity we get a discounted rate
> Compute
    > Charged for active warehouses per hour
    > Depending on the size of the warehouse
    > Billed by second (minimum of 1 min.)
    > Charged in Snowflake credits
> We only get charged for data moving out of Snowflake (not bringing data into Snowflake)
> We are not charged if we want to share data with another snowflake account within the same region using the same cloud provider

Roles in snowflake
------------------
> A user gets assigned different roles and a role has one or many privileges
> Custom roles can be created that can inherit privilges from other roles
> Important roles (hierarchy from top to bottom):
    > ACCOUNTADMIN
        > Combines everything that SYSADMIN and SECURITYADMIN can do
        > Top level role in the system
        > Should be granted only to a limited number of users
    > SECURITYADMIN
        > USERADMIN role is granted to SECURITYADMIN
        > Can manage users and roles
        > Can manage any object grant globally
    > SYSADMIN
        > Create warehouses and databases (and more objects like schemas)
        > Recommended that all custom roles are assigned
    > USERADMIN
        > Dedicated to user and role management only
        > Can create users and roles
    > PUBLIC
        > Automatically granted to every user
        > Can create own objects like every other role

Loading data
------------
> Bulk loading (batch loading)
    > Most frequent method of data loading for large amount of data
    > Uses warehouses
    > Loading from stages
    > COPY command
    > Transformations are possible
> Continuous loading
    > Designed to load small volumes of data
    > Loading takes place automatically once they are added to stages
    > Latest result is available for analysis
    > Uses Snowpipe (serverless feature)

Understanding stages
--------------------
> These are database objects that have properties like location of data files where data can be loaded from
> Categories:
    > External stage
        > Facilitated by an external cloud provider like AWS S3, GCP Bucket, Azure Blob Storage
        > Can be created using `CREATE STAGE` command which has properties like URL, access settings, etc.
        > It can incur additional costs if stage is present in a different region or cloud platform
    > Internal stage
        > local storage maintained by Snowflake

> Load unstructured data
    > Create a stage
    > Load raw data into a table with one column of type VARIANT (this can handle JSONs)
    > Analyse and parse
    > Flatten the hierarchical data and load

> Performance optimization
    > Traditional ways:
        > Add indexes and primary keys
        > Create table partitions
        > Analyze the query execution table plan
        > Remove unnecessary full table scans
    > In snowflake:
        > Automatically managed micro-partitions
        > Assign approriate data types
        > Sizing virtual warehouses
        > Cluster keys
        > Create dedicated virtual warehouses separated according to different workloads
        > Scaling up for known patterns of high work load (process complex queries)
        > Scaling out dynamically for unkown pattern of work load (more concurrent users/queries)
        > Maximize automatic cache usage

> Caching
    > Automatic process to speed up queries
    > If query is executed twice, results are cached and can be re-used
    > Results are cached for 24 hours or until underlying data has changed
    > Ensure similar queries go on the same warehouse

> Clustering
    > Cluster key
        > Subset of rows to locate the data in micro partitions
        > For large tables this improves the scan efficiency in our queries
    > Snowflake automatically maintains these cluster keys
    > We can manually customize cluster keys
    > Clustering is mainly for very large tables (multiple TBs)
    > Columns that are used most frequently in WHERE clause should be used for clustering
    > If you use filters on two columns then the table can also benefit from two cluster keys
    > Columns that are frequently used in joins

> Snowflake & AWS (load data from S3 bucket to Snowflake)
    > Go to IAM and create a role
    > Select 'Another AWS Account'
    > Get AWS account ID
    > Enable 'Require External ID'
    > Grant S3 related permissions
    > Create integration object in Snowflake
    > Edit trust relationship for IAM role (paste values from integration object properties using DESC)
        > STORAGE_AWS_IAM_USER_ARN ==> 'AWS key' inside trust relationship JSON
        > STORAGE_AWS_EXTERNAL_ID ==> 'ExternalId' inside trust relationship JSON

> Snowflake & Azure (load data from storage account to Snowflake)
    > Create integration object
    > Grant permission to snowflake using AZURE_CONSENT_URL
    > Add role assignment to storage account

> Snowflake & GCP (load data from GCS bucket)
    > Create integration object
    > Add member to GCS buckets (STORAGE_GCP_SERVICE_ACCOUNT) and assign storage related role

> Unload data
    > Data from a snowflake table can be exported out as a file to a stage (S3, storage account or GCS)
    > Need to have write access to the bucket/container

> Snowpipe
    > Enables loading once a file appears in a bucket
    > If data needs to be available immediately for analysis
    > Snowpipe uses serverless features instead of warehouses
    > High level steps:
        > Create a stage object
        > Create and test COPY command
        > Create a pipe
        > Setup S3 notification (to trigger snowpipe)
            > Copy value of notification_channel (Using DESC on pipe)
            > In S3 bucket, go to event notifications and create one
            > Enable 'all object create events'
            > In Destination, select SQS queue and enter ARN
    > Ingestion can take upto 1 minute
    > Indeed snowpipe is not intented for batch loading but for continous loading. For batch loading we should just use the COPY command
