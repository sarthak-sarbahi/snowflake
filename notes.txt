Snowflake interface (snowsight):
--------------------------------
> Worksheets: Write/run queries and code

Architecture:
-------------
> Data is compressed and stored in blobs (AWS S3, Azure Blob Storage, GCP Buckets)
> Virtual warehouses: A set of virtual compute engines query the data stored in blobs (also perform MPP wherein multiple servers come together to process big data)
> Cloud providers help to manage all of this: Managing infrastructure, access control, security, optimizer and metadata
> Virtual warehouse sizes (no. of servers): 
    > XS - 1, S - 2, M - 4, L - 8, XL - 16, 4XL - 128
    > Billed by minute (min. 1 minute)
    > Multi-clustering: When you a multiple queries to execute but 1 virtual warehouse is not sufficient, then you can use multi clustering which brings in multiple virtual warehouses together (like a cluster) so that queries dont get queued up

Scaling policy
--------------
> Scaling is required when the workload is high (more queries to be processed) 
> Policies
    > Standard:
        > This is the default scaling policy
        > Prevents/minimizes queuing by favoring starting additional clusters over conserving credits
        > Cluster starts immeditely when either a query is queued or the system detects that there are more queries than can be executed by the currently available clusters
        > Cluster shuts down after 2 to 3 consecutive successful checks (performed at 1 minute intervals) which determine whether the load on the least-loaded cluster could be redistributed to the other clusters 
    > Economy
        > Conserves credits by favoring keeping running clusters fully loaded rather than starting additional clusters
        > May result in queries being queued and taking longer to complete
        > Only if the system estimates there's enough query load to keep the cluster busy for at least 6 minutes
        > After 5 to 6 consecutive successful checks

Snowflake editions
------------------
> Standard: Introductory level
> Enterprise: Additional features for the needs of largescale enterprises
> Business Critical: Even higher levels of data protection for organisations with extremely sensitive data
> Virtual Private: Highest level of security

Snowflake pricing
-----------------
> Pay only what you need
> Scalable amount of storage at affordable cloud price
> Pricing depending on the region and cloud platform
> Compute and storage costs decoupled
> Storage
    > Monthly storage fees
    > Based on average storage used per month
    > Cost calculated after compression
    > If we pay upfront for pre-defined capacity we get a discounted rate
> Compute
    > Charged for active warehouses per hour
    > Depending on the size of the warehouse
    > Billed by second (minimum of 1 min.)
    > Charged in Snowflake credits
> We only get charged for data moving out of Snowflake (not bringing data into Snowflake)
> We are not charged if we want to share data with another snowflake account within the same region using the same cloud provider

Roles in snowflake
------------------
> A user gets assigned different roles and a role has one or many privileges
> Custom roles can be created that can inherit privilges from other roles
> Important roles (hierarchy from top to bottom):
    > ACCOUNTADMIN
        > Combines everything that SYSADMIN and SECURITYADMIN can do
        > Top level role in the system
        > Should be granted only to a limited number of users
    > SECURITYADMIN
        > USERADMIN role is granted to SECURITYADMIN
        > Can manage users and roles
        > Can manage any object grant globally
    > SYSADMIN
        > Create warehouses and databases (and more objects like schemas)
        > Recommended that all custom roles are assigned
    > USERADMIN
        > Dedicated to user and role management only
        > Can create users and roles
    > PUBLIC
        > Automatically granted to every user
        > Can create own objects like every other role

Loading data
------------
> Bulk loading (batch loading)
    > Most frequent method of data loading for large amount of data
    > Uses warehouses
    > Loading from stages
    > COPY command
    > Transformations are possible
> Continuous loading
    > Designed to load small volumes of data
    > Loading takes place automatically once they are added to stages
    > Latest result is available for analysis
    > Uses Snowpipe (serverless feature)

Understanding stages
--------------------
> These are database objects that have properties like location of data files where data can be loaded from
> Categories:
    > External stage
        > Facilitated by an external cloud provider like AWS S3, GCP Bucket, Azure Blob Storage
        > Can be created using `CREATE STAGE` command which has properties like URL, access settings, etc.
        > It can incur additional costs if stage is present in a different region or cloud platform
    > Internal stage
        > local storage maintained by Snowflake

> Load unstructured data
    > Create a stage
    > Load raw data into a table with one column of type VARIANT (this can handle JSONs)
    > Analyse and parse
    > Flatten the hierarchical data and load

> Performance optimization
    > Traditional ways:
        > Add indexes and primary keys
        > Create table partitions
        > Analyze the query execution table plan
        > Remove unnecessary full table scans
    > In snowflake:
        > Automatically managed micro-partitions
        > Assign approriate data types
        > Sizing virtual warehouses
        > Cluster keys
        > Create dedicated virtual warehouses separated according to different workloads
        > Scaling up for known patterns of high work load (process complex queries)
        > Scaling out dynamically for unkown pattern of work load (more concurrent users/queries)
        > Maximize automatic cache usage

> Caching
    > Automatic process to speed up queries
    > If query is executed twice, results are cached and can be re-used
    > Results are cached for 24 hours or until underlying data has changed
    > Ensure similar queries go on the same warehouse

> Clustering
    > Cluster key
        > Subset of rows to locate the data in micro partitions
        > For large tables this improves the scan efficiency in our queries
    > Snowflake automatically maintains these cluster keys
    > We can manually customize cluster keys
    > Clustering is mainly for very large tables (multiple TBs)
    > Columns that are used most frequently in WHERE clause should be used for clustering
    > If you use filters on two columns then the table can also benefit from two cluster keys
    > Columns that are frequently used in joins